{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fe8352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.exceptions import ProxyError, RequestException\n",
    "from random import shuffle\n",
    "\n",
    "# Replace with your list of proxy IP addresses and ports\n",
    "PROXY_LIST = [\n",
    "   \n",
    "\n",
    "\"27.77.148.97:14041\",\n",
    "\"78.110.195.129:7080\",\n",
    "\"47.90.82.199:3128\",\n",
    "\"37.235.53.208:18888\",\n",
    "\"139.162.220.253:10881\",\n",
    "\"103.130.61.61:8081\",\n",
    "\"78.110.195.241:7080\",\n",
    "\"151.236.14.178:9090\",\n",
    "\"74.82.50.155:3128\"\n",
    "\n",
    " # Add more proxies as needed\n",
    "]\n",
    "\n",
    "# Define website URL and number of pages\n",
    "TARGET_URL = \"https://news.google.com/home?hl=en-IN&gl=IN&ceid=IN:en\"\n",
    "NUM_PAGES = 5\n",
    "\n",
    "def make_request(url, proxy):\n",
    "    \"\"\"\n",
    "    Makes a request to the given URL using the provided proxy.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, proxies={\"http\": proxy, \"https\": proxy}, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except ProxyError as e:\n",
    "        print(f\"Proxy error with {proxy}: {e}\")\n",
    "        return None\n",
    "    except RequestException as e:\n",
    "        print(f\"Request error: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return None\n",
    "\n",
    "def scrape_with_proxies():\n",
    "    \"\"\"\n",
    "    Scrapes multiple pages of the target website using rotating proxies.\n",
    "    \"\"\"\n",
    "    # Shuffle the list for random proxy selection\n",
    "    shuffle(PROXY_LIST)\n",
    "\n",
    "    for page_number in range(1, NUM_PAGES + 1):\n",
    "        target_page = f\"{TARGET_URL}{page_number}\"\n",
    "        current_proxy = PROXY_LIST.pop(0)\n",
    "\n",
    "        print(f\"Scraping page {page_number} with proxy: {current_proxy}\")\n",
    "\n",
    "        # Make request with the current proxy\n",
    "        page_content = make_request(target_page, current_proxy)\n",
    "\n",
    "        # Check for successful response and process content\n",
    "        if page_content:\n",
    "            print(f\"Page {page_number} content:\\n{page_content}\\n\")\n",
    "\n",
    "        # Add the proxy back to the list for potential reuse\n",
    "        PROXY_LIST.append(current_proxy)\n",
    "\n",
    "# Run the scraping function\n",
    "scrape_with_proxies()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
